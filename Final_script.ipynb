{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import nltk \n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "import clean_text2 as ct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    with open(file_name,'r',encoding='utf8') as f:\n",
    "        word_vocab = set() # not using list to avoid duplicate entry\n",
    "        word2vector = {}\n",
    "        for line in f:\n",
    "            line_ = line.strip() #Remove white space\n",
    "            words_Vec = line_.split()\n",
    "            word_vocab.add(words_Vec[0])\n",
    "            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n",
    "    print(\"Total Words in DataSet:\",len(word_vocab))\n",
    "    return word_vocab,word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 667 entries, 0 to 666\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       667 non-null    object\n",
      " 1   1       667 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 10.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data_set = pd.read_csv('covid_dataset_csv.csv')\n",
    "data_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in DataSet: 400000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "data_set = pd.read_csv('covid_dataset_csv.csv',encoding='latin1')\n",
    "vocab, w2v = read_data('glove.6B.50d.txt')\n",
    "w2v['covid'] = w2v['coronavirus']\n",
    "w2v['covid-19'] = w2v['coronavirus']\n",
    "w2v['covid19'] = w2v['coronavirus']\n",
    "w2v['corona virus'] = w2v['coronavirus']\n",
    "all_words = w2v.keys()\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(sym_spell,input_term):\n",
    "    suggestions = sym_spell.lookup_compound(input_term, max_edit_distance=2)\n",
    "    sentence = []\n",
    "    print(len(suggestions))\n",
    "    for suggestion in suggestions:\n",
    "        sentence.append(str(suggestion).split(',')[0])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(u,v):\n",
    "    numerator_ = u.dot(v)\n",
    "    denominator_= np.sqrt(np.sum(np.square(u))) * np.sqrt(np.sum(np.square(v)))\n",
    "    return numerator_/denominator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_similarity1(q1,data_set):\n",
    "    similarity=[]\n",
    "    q1_clean = [ct2.getClearReview(i) for i in [q1]][0].split()\n",
    "    for i in range(data_set.shape[0]):\n",
    "        q2 = data_set.iloc[i]['1']\n",
    "#         print(q2)\n",
    "        q2_clean = [ct2.getClearReview(i) for i in [q2]][0].split()\n",
    "        ans = 0;\n",
    "        for w1 in q1_clean:\n",
    "            max1_ = 0\n",
    "            for w2 in q2_clean:\n",
    "                if w2 not in vocab:\n",
    "                    continue\n",
    "                sim1 = cos_sim(w2v[w1],w2v[w2])\n",
    "                max1_ = max(sim1,max1_)\n",
    "            ans += max1_\n",
    "        similarity.append(ans/len(q1_clean))\n",
    "    return similarity\n",
    "def overall_similarity0(q1,data_set):\n",
    "    similarity=[]\n",
    "    q1_clean = [ct2.getClearReview(i) for i in [q1]][0].split()\n",
    "    for i in range(data_set.shape[0]):\n",
    "        q2 = data_set.iloc[i]['0']\n",
    "#         print(q2)\n",
    "        q2_clean = [ct2.getClearReview(i) for i in [q2]][0].split()\n",
    "        ans = 0;\n",
    "        for w1 in q1_clean:\n",
    "            max1_ = 0\n",
    "            for w2 in q2_clean:\n",
    "                if w2 not in vocab:\n",
    "                    continue\n",
    "                sim1 = cos_sim(w2v[w1],w2v[w2])\n",
    "                max1_ = max(sim1,max1_)\n",
    "            ans += max1_\n",
    "        similarity.append(ans/len(q1_clean))\n",
    "    return similarity\n",
    "def overall_similarity(q1,data_set):\n",
    "    similarity0 = overall_similarity0(q1,data_set)\n",
    "    similarity1 = overall_similarity1(q1,data_set)\n",
    "    similarity = 2*np.array(similarity0)+np.array(similarity1)\n",
    "#     similarity = np.array(similarity0)\n",
    "#     print(similarity0.shape)\n",
    "#     print(similarity1.shape)\n",
    "    print(similarity.shape)\n",
    "    similarity = list(similarity)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(sym_spell,input_term):\n",
    "#     input_term = str(spelling_correction(sym_spell,input_term)[0])\n",
    "    input_term = input_term.replace('corona virus','coronavirus')\n",
    "#     input_term = input_term.replace('ovid' , 'coronavirus')\n",
    "    similarity = overall_similarity(input_term,data_set)\n",
    "    print(similarity[np.argmax(similarity)])\n",
    "    s = \"\"\"We don\\'t seem to have any information on that. Would you like to perform a web search instead?\"\"\"\n",
    "    if similarity[np.argmax(similarity)]<2:\n",
    "        return s\n",
    "    return data_set.iloc[np.argmax(similarity)]['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667,)\n",
      "1.2912991414241597\n",
      "We don't seem to have any information on that. Would you like to perform a web search instead?\n"
     ]
    }
   ],
   "source": [
    "print(answer(sym_spell,'What is melody so chocolaty?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('symspell.pickle','wb')\n",
    "pickle.dump(sym_spell, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('symspell.pickle','rb')\n",
    "sym_spell = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('w2v.pickle','wb')\n",
    "pickle.dump(w2v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('w2v.pickle','rb')\n",
    "w2v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'can eating meat causes coronavirus ?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(sym_spell,'What are causes of coronavirus?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dataset.pickle','wb')\n",
    "pickle.dump(data_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('dataset.pickle','rb')\n",
    "dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vocab.pickle','wb')\n",
    "pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vocab.pickle','rb')\n",
    "vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(str(spelling_correction(sym_spell,'hello')[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
